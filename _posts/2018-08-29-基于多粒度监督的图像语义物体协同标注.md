---
layout:     post
title:      基于多粒度监督的图像语义物体协同标注
subtitle:   论文笔记
date:       2018-08-29
author:     crazyang
header-img: 
catalog: true
tags:
    - AI
---

本笔记首先是对论文进行了一定的概述，然后尝试进行复现，目前还未实现


- **论文 Collaborative Annotation of Semantic Objects in Images with Multi-granularity Supervisions**
- **链接 [https://arxiv.org/pdf/1806.10269.pdf](https://arxiv.org/pdf/1806.10269.pdf)**
- **源码 [http://cvteam.net/projects/2018/MM/colt.html](http://cvteam.net/projects/2018/MM/colt.html)**

# 论文概述

## 介绍

作者首先提出了语义图像分割中两个最耗时的类型：
>1、用多边形逼近明显的非多边形边界（计算机标注更适合处理此类任务）；
>2、目标和干扰物的边界相似模糊（人标注更适合此类任务）。

由此作者提出了让计算机和人协同标注的方法。现在的协同方法分为两种：

>1、**独立的机器决策（Agent-decision）**：人先标记画出bounding box，然后利用深度学习算法，例如GrabCut和CNN。这种方法的一个主要缺点就是：这种结果是由机器做出的决策，确信度不高，不能够作为ground-truth。
>2、**独立的人决策（Human-decision）**：先由机器大致生成一个粗略的标注结果，然后人进行修正。这种方法的好处是机器越强大，人越能节省时间，同时结果也能作为ground-truth。缺点在于这样的机器模型都是经过了预训练的，这样在面对新的不一般的数据集时就没办法实现了；同时机器还会反复出现同一个错误。

在以上两种方法的基础上，作者提出了动态协同标注方法，机器根据人类提供的一些样例进行自动标注，这些样例的需求可以是很少的，然后人类根据机器自动标注的进行修正，修正好的会被记录下来，这样机器具有了不断自我学习的能力，最后的错误率会越来越低。主要目标是将以前数据集中标记的图像(例如ImageNet)快速转换为语义对象的像素掩码。

## 实现过程
1. 建立一个弱字典（同样标签的图片），一个强词典（之前标注过的语义对象）。
首先根据每一类图像的语义标签计算语义相似性，然后根据图像特征计算每类图像之间的视觉相似性（用的是**DPN模型**，将图片转化为2688维的特征向量），联合得到每类图像之间的总相似性。选择相似性大于 0.95 的类别作为当前类别的稀疏编码字典。有像素级标注图像类别的特征的作为强字典，没有像素级标注图像类别特征的作为弱字典。
为了便于计算，使用**PCA算法**将2688维的向量降维到100。

2. 根据这两个词典机器，机器首先从每个图像中提取一组目标建议，然后根据稀疏编码长度测量它们与图像标签的关系。
编码对象是用 **MCG 算法**对图像提取出来的排在前 200 的图像 proposal。由于图像的分辨率和像素密度很高，作者为减少人工点击次数，借助超像素块进行操作。作者把 proposal 编码长度映射到超像素块并归一化得到每个超像素块的属于前景的可能性值，选择大于 0.4 的作为前景，剩下的作为背景，得到机器初始化结果。

3. 对于生成的像素块，人类用鼠标点击交互，以使得像素块包含所有合适的位置。
根据机器初始化结果，标注者进行修正：如果前后背景错误直接点击左键，如果边缘分割错误，首先点击右键进行分裂成更小的超像素块，然后点击左键。在人工修正的过程，机器会自动保存点击超像素块的 3 邻域特征用于后续的机器自动修正。 

4. 机器再自动修正
选择在阈值 0.4 上下 0.15 范围内的超像素块，用人工修正保存的超像素块 3 邻域特征进行稀疏编码，得到这些超像素块的编码长度，归一化选择大于 0.95 的超像素块进行前景背景在初始化基础上进行反转。得到机器自动修正结果。随着人工标注的结果越多，机器能学的越精确，自动化修正结果会更好。

## 实验

作者选取了 40 个图像类别：
>在 ImageNet 数据集 1000 类中并且和 MSCOCO 有相同标签的 10 个类别
>在 ImageNet 数据集 1000 类中并且和MSCOCO有不同标签的 10 个类别
>不在 ImageNet 数据集 1000 类中并且和 MSCOCO 有相同标签的 10 个类别
>不在 ImageNet 数据集 1000 类中并且和 MSCOCO 有不同标签的 10个 类别。

作者选择 10 个年龄在 20-28 周岁之间的标注者进行标注，每个标注者用 LabelMe 进行标注 4 个图像类别。得到 LabelMe 的标注结果，作为本文的 ground-truth。为了比较作者方法的自动分割结果，作者和当前自动分割处于领先水平的两个方法：DeepMask 和 SharpMask 进行比较。发现这两个方法的结果都明显低于 Colt 的初始化结果。

## 贡献
提出了一种人-机器协作的方法，将以前数据集中标记的图像转换为语义对象的像素掩码，同时创建了一个动态机器，可以在标注过程中逐步学习。与当前能作为 Ground-Truth 的人工标注方法 LabelMe 的标注结果相比，作者标注工具 collaborative tool (Colt) 的标注结果 f-measure 值能够达到 91.21%，同时作者的标注工具能节约 50% 的人工标注时间。


----------

# 论文复现

首先从源码地址下载文件，一共有四个文件（一个是标注工具，三个是图片集），标注工具很大，下载特别慢。
解压标注工具后是一个VS的工程，初次执行你需要配置OpenCV（相信你会）和[Qt](https://mp.csdn.net/postedit/81939610)（点击链接参考我的方法）

然后执行tag2pixel项目，会打开一个图形界面，如下图

同时我还实现了Facebook提供的开源工具DeepMask和SharpMask工具，这两个是自动标注的起点，只需要传进去一张图片，就能自动分割好，如何实现呢？

**参考我写的[如何使用DeepMask和SharpMask](https://blog.csdn.net/yzy_1996/article/details/81909444)**


关于论文的复现目前还存在问题是：
>作者只提供了图片，缺少相应的标签
>一些自动化的工作不知道原理
>提供的软件工具没有说明指南，不会用

期待更新
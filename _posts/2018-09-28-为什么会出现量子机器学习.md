---
layout:     post
title:      为什么会出现量子机器学习
subtitle:   前沿
date:       2018-09-28
author:     crazyang
header-img: img/quantum.jpg
catalog: true
tags:
    - Quantum
---


## GPU
现在研究机器学习和神经网络的人都知道，实验的运算离不开GPU（图形处理器），当面对类型高度统一的、相互无依赖的大规模数据和不需要被打断的纯净的计算环境时，GPU的并行运算能力得以发挥出来。并行计算打个比方就是原来是单车道，只能一次通行一辆车，现在变成四车道了，就能一次通行四辆车了。

有一个数据表明，当你花同样多的钱的时候，买GPU得到的计算能力是买CPU的15倍。

[一个有趣的视频可以用来对比GPU和CPU](https://v.youku.com/v_show/id_XNjY3MTY4NjAw.html?spm=a1z3jc.11711052.0.0&isextonly=1)

<iframe 
    width="720" 
    height="450" 
    src="http://player.youku.com/embed/XNjY3MTY4NjAw"
    frameborder="0" 
    allowfullscreen>
</iframe>



摩尔定律大家一定都听说过，对于半导体器件，晶体管的数量在变得越来越少，性能也在变得越来越强，但是它的发展会遇到瓶颈，当我们把芯片做得越来越小时，会出现量子效应，器件将不能正常工作，所以摩尔定律会不再成立了。

为了追求新的运算性能增加，科学家们将视野投向了量子机器学习。

# 量子计算机

关于量子计算机的提出，有这样一段故事：
>一小块固体含的原子数目是10的23次方量级，如果要求解这么多粒子的薛定谔方程（量子力学的基本方程）基本是没有希望的，这样就没法用计算机模拟，从而预测它们的性质。物理学家[费曼](https://baike.baidu.com/item/%E7%90%86%E6%9F%A5%E5%BE%B7%C2%B7%E8%B4%B9%E6%9B%BC)在1982年想到既然我们想模拟的对象是量子的，那么我们为什么不能建一个量子的计算机来模拟呢？量子计算机就是在这样的背景下提出来的。

 量子带来的优点：

 1. 量子态具有叠加性，传统计算机运算状态只能是0或1，但量子具有多态性，也就给计算机带来更多运算的可能性。
 2. 量子具有相干性，可从一个或多个量子状态推出其它量子态，这在储存当中会起到牵一发而动全身的效果。

目前的发展情况是：

 1. 2011年5月11日加拿大的D-Wave 系统公司发布了一款号称“全球第一款商用型量子计算机”的计算设备“D-Wave One”，含有128个量子位。
 2. 2013年5月D-Wave 系统公司宣称NASA和Google共同完成了一台采用512量子位的D-Wave Two量子计算机，运行特定算法时比传统计算机快上亿倍。
 3. 2016年8月，美国马里兰大学学院市分校发明世界上第一台由5量子比特组成的可编程量子计算机。
 4. 2017年5月，中国科学院宣布制造出世界首台超越早期经典计算机的光量子计算机，虽然还是缓慢但已经逐步跨入实用价值阶段。
 5. 2017年7月，美国研究人员宣布完成51个量子比特的量子计算机模拟器。

目前阿里巴巴，腾讯，华为也都有布局量子计算机，前路漫漫，还有很多难关需要被攻克，但未来是光明的。

## 量子机器学习
说了半天的量子，那和机器学习又是如何沾上关系的呢。
先用一张图展示一下，从图中可以看到，比如传统的模拟退火算法，科学家在量子领域也想出了量子模拟退火。
![在这里插入图片描述](https://img-blog.csdn.net/20180928142126376?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3l6eV8xOTk2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

正如前面讲到了，机器学习在面对庞大的神经网络时，消耗了很大的运算力，而GPU的并行运算可以提供加速，而传统的半导体芯片会受到物理上的瓶颈限制，量子计算机的出现正好可以打破这一瓶颈。

感兴趣的同学可以去查阅Github上的一个资料整理的项目，[awesome-quantum-machine-learning](https://github.com/krishnakumarsekar/awesome-quantum-machine-learning)